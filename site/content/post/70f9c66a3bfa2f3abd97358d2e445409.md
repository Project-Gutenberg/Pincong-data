---
layout: default
Lastmod: 2020-08-04T17:14:30.125415+00:00
date: 2020-08-04T17:14:30.125182+00:00
title: "说说 Big data 的出现对于现存的 survey data 带来的含义？"
author: ""
tags: [抽样,样本,普查,机顶盒,调研]
---


    
### 知乎用户 chenqin​ 发表
    
谢谢邀请。想象你面前有一群大学生，你需要了解他们的信息，那么对于信息的要求，不外乎三大类：

1，全面，能够反映这群人的整体情况  
2，深入，让你知道一些更多维度且准确的信息  
3，及时，你肯定希望这样的信息能尽快到手边来

这三项要求，满足其中一项，还是比较容易的。

你可以成为他们的辅导员，从大家的注册信息、历次考试成绩中，逐渐了解这个群体的大致情况，满足全面性；  
你可以成为他们的一员，交几个好朋友，花上几天时间了解部分信息——比如其中某一个贫困生其实并不贫困，满足准确性；  
或者你是一个第二天就要提交转系申请的同学，你坐在电脑前一下午，搜索了这个群体中一些人的知乎和微博，大致了解了今后朝夕相处的同学是什么样的人。

但这样的要求提高到两项时，就不是那么容易了，由此派生出了各种方法。比如，要同时满足全面性和准确性，你可能必须开展一次小规模人口普查，让每位同学都做一份问卷，但花去你半个月时间调查，再花一星期时间做数据整理和分析；要同时满足及时性和准确性，你必须从这个群体中设计一项抽样准则，抽取几个有代表性的同学出来，进行准确了解，但必须注意抽样是否科学，信息是否会有偏。这两种数据，事实上就是 census data（普查数据）和 survey data。

中国的 GDP 数据正是在这样的要求下进行的。国家统计局每 5 年进行一次经济普查，确定中国的基本单位情况，但每次普查需要一年时间，数据的清理再要花一年（全面而准确，但不能做到及时）；在普查数据的基础上，确定抽样框，每年进行规模以上企业的数据直报和规模以下企业的抽样调查（准确而及时，但不能做到全面）。

将 census 和 sampling survey 两类方法结合在一起，就构成了我们所见的大部分统计数据的基础。这两种方法，每种都至少满足了两个要求，那么两者的结合是否就能同时满足三个要求了呢？

不能。

举个例子（[中国的人口数据 - Clean Data - 知乎专栏](http://zhuanlan.zhihu.com/p/20222971)），上海在全国人口普查的基础上，每年抽样千分之一点五的人口，计算他们的变动，结果在 2010 年低估 289 万人口。  
再有，中国在第三次经济普查结束后，中国的 GDP 总量被向上调整，原因是之前的规下企业抽样漏掉了大量服务业企业（[马建堂回应 GDP 数据修订：3% 多一点](https://link.zhihu.com/?target=http%3A//news.xinhuanet.com/fortune/2014-12/16/c_127308537.htm)）。

钱花了，时间花了，大量人力也花上去了，为什么没法得到准确结果？原因就在于，我们对总体的了解，对总体的刻画，过于滞后了——每年抽取千分之一点五的人口，哪些人口的千分之一点五？每年抽取规下企业进行调查，可是样本框却是五年前的普查构成的，又如何抽取到新企业？

于是，我们就不得不在 survey data，census data 之外找到第三种选择，Big data，好的 Big data 能够满足全面性和及时性，但在准确性上不能要求太高。大数据的产生和收集过程成本相对较低，但却是基本完整覆盖的。比如，要调查某区域的失业率，你可以让所有的居委会大妈都四处查看，每半年一次报告，谁谁谁在家里蹲了半年啦，看来是工作丢了；或者派出调查队，每个月进行一次入户劳动力动态抽样调查，估算出一个失业率；或者是你可以直接使用这个区域的移动终端数据，计算他们在居住区到楼宇之间的往返移动状况，并在此基础上实时产生这个区域的 “失业” 情况。

当然，用移动终端的行为来判断失业，肯定有不准确的地方，比如是不是有人搬走啦，是不是有人有三台手机，是不是有人开始在家办公等等。但无论如何，这样的大数据非常及时地向我们揭示了群体中变动的特征，他们全面、及时，却又包含了许多噪音，这样的数据虽然不能直接进行政策研究和学术研究，但他提示了我们下一步的政策和研究方向可能去值得关注。

再比如之前人口和 GDP 的例子，大数据虽然不能直接产生结果，但是通过多种来源的大数据，移动终端数据、网购数据、企业招聘数据等，我们可以用相对较低的成本，较短的时间，了解到总体的变化大致是怎样的，告诉我们需要在哪里进行重点抽样和样本框的扩张。

现在我们知道了，census data、survey data 和 big data 其实都是在数据的三项需求中满足了两项，他们各自有一条短板。将大数据和普查数据、深入的微观调查数据结合起来，相互印证，我们可以得到更加准确的信息。

那么，有没有三项需求全部满足的数据呢？请看下图。  



![](https://images.weserv.nl/?url=https%3A//pic4.zhimg.com/8dd4e89e61308d6d0afb7d500e258035_r.jpg%3Fsource%3D1940ef5c)
    
    
    
    
### 知乎用户  慧航​ 发表
    
谢邀。这是个非常有意思的问题，也是我最近出于工作的原因在思考的问题。抛砖引玉，写点感想。

在之前的一个回答中（[https://www.zhihu.com/question/37870042](https://www.zhihu.com/question/37870042)），我提到了 quality-quantity tradeoff，我想这个应该是理解两种数据的关键。对于传统的调查数据而言，由于问卷设计、抽样设计在统计学里面已经有非常完善的理论，因而获得比较理想的样本应该说技术上是不难的。但是问题是这种数据成本太高，想要高质量的样本，必须砸钱。无论是学术还是商业用途，成本是制约调查数据的关键因素。

而相对于调查数据而言，现在流行的所谓大数据可以以非常低的成本获得大量的数据，然而数据中的噪声也是很多的。很多时候我们以技术手段获得的数据，要么有样本偏差，要么有度量误差，要么关键的变量压根看不到，等等等等，会有很多各种各样的问题。究其原因，是因为很多的大数据获取手段是被动的获取，而非调查数据那样是主动的去索取。

听过我上次 live 的朋友应该记得我讲过一个推销理财产品的案例。也许我们可以看到一家银行所有客户的所有可获得数据，然而关键的变量，比如银行存款余额（因为不知道储户在其他银行的存款），是一个有非常大的度量误差的数据。或者说，如果仔细考虑用户需求，在本行已经有高额存款的用户银行已经推销过了，发现那些在其他银行有高额存款的用户才是用户的真实需求。而数据的这些问题给我们的分析带来了蛮大的困难。

当然，技术上可以部分解决这些问题，不过如果我们可以结合调查数据呢？比如，在银行可以承受的成本范围内，针对已经有的数据做好抽样设计，做一些电话回访帮助把关键的变量补回呢（存款余额太敏感可以转而调查消费、月收入）？也许一些半监督学习的方法或者其他有结构假设的模型可以适用。

我个人没有什么偏见，碰到问题的时候，如果花一点钱能解决技术上难度比较大的问题，花点钱做调查比费心思改进技术做模型要来的实惠的多。能够将两种数据漂亮的结合起来，取长补短，以最低的成本获得最好的精度应该是个不错的思路。
    
    
    
    
### 知乎用户 马振凯​ 发表
    
我也长期对这个问题困惑过，因为大部分现在做大数据的人仅仅是获取数据的方式方便了，传统统计小数据的知识还不具备，总是认为数据越大越好，对小数据有点先天的歧视。有时候都还觉得我数据全拿到的，不存在你们小数据时候的抽样问题了，彻底颠覆，欧耶。

二者如何结合我也不知道如何弥合，不过最近系里的《社会》杂志做了一期大数据的专题，有一个博士做了一个电视收视率的研究的讲座，让我对这个问题稍微有了些了解了。  
[欢迎访问社会杂志](https://link.zhihu.com/?target=http%3A//www.society.shu.edu.cn/CN/volumn/volumn_974.shtml)  
这是我们杂志网址，论文可以开放下载，但是应该过段时间才会发表出来，毕竟双月刊。

**首先明确，统计数据 statistic 这个词，词根是 state 国家，统计一直是西方现代政治的重要治理术之一。你也可以想象，农耕封建时代，就国王才需要统计。方便提取治理区域的信息。所以我们算均值和方差。一个抽取主要集中信息，一个抽取主要变异信息。这个统计的活一般国家和大机构才能做。**

  

**然而互联网来了，爬虫技术后和计算机管理生产后，获取数据的成本和时间小了多。应该说这个大数据是统计的民主化运动。特别是对于预测来说，数据越多，往往结果越准。但是对于用数据说明一个道理，发展一个理论来说，由于要知道数据下面的故事和机制，所以数据越多往往不见得越好。**

专业且简单地不蒙人抖机灵地说，小数据和大数据的区别可以用这张图表示。

假如现实是个美女长这样  



![](https://images.weserv.nl/?url=https%3A//pic4.zhimg.com/51aee42ae9a3510c0699e98a2151e1db_r.jpg%3Fsource%3D1940ef5c)

  
那么小数据的方法了解她的样子是这样的  



![](https://images.weserv.nl/?url=https%3A//pic2.zhimg.com/a4f492b9bcc2d49ad3785f01585f6475_r.jpg%3Fsource%3D1940ef5c)

编制一个统一的抽样结构，然后随机抽样，了解每个局部的小样本，这样因为是随机抽样的，尽管数据小，但是是有结构的，你大概可以看出来这是个美女，而且知道后面背景是空的，整个图片虽然有细节不清楚，但是大概可以知道情况，**起码这是个女的，而且照片背景很空**。  
而大数据了解的照片是这样的  



![](https://images.weserv.nl/?url=https%3A//pic1.zhimg.com/d3815d4e11c6fc7cda593e231dcd9961_r.jpg%3Fsource%3D1940ef5c)

大数据可以详细获得某些局部，比小数据成本低，然后小，细节很清楚。但是问题是，这些局部不知道在总体中的结构是怎么样的，而且拼接也很困难。**尽管你清楚知道这个人眼睛耳朵什么样，但是这个人整体是不是个女的? 背景空不空？经常不知道**

举个那个我最近知道的例子，研究上海电视收视率，是多少。

小数据的方法，所有居民点随机抽样，上门访问或者装专门的收视监测机器，例如上海市 200 户。然后用这 200 户的收视来推断上海整个城市的收视情况，如果随机抽样严谨的话，这个结论是显著的话，确实 200 样本足够代表上海市的收视情况，而且可以很精准，精确到 1% 比较容易做到。数据监测仪的数据对于这 200 户的行为测量也是很精准的。而且可以顺便收集下这些家庭的家庭成员、收入、年龄等基本的人口统计学数据。缺点是 **1 所有居民点信息的抽样框很难做，**例如一些临时居住点，工厂厂房的电视很容易在抽样时候忽略。**2 这样直接问的指标，例如收入，问往往大家回答不诚实，**毕竟不是直接对行为的测量。3、**了解全国的数据成本太大，测量仪毕竟贵，上门装和反馈的调查员一天要给 150 吧？**一般只能专业大公司来做。

大数据的方法，找电视供应商，例如 tcl、长虹、康佳、乐视。他们有全的各自客户的数据，各种做工作，然后全部弄下来。这样全国数据很好获得，几个人也能调查出来。好处是样本量大大于随机小数据的方法，而且基本是全的，成本低，不用装测量仪器。问题也很多

1、数据的结构和口径不一样，例如我国有卫星电视、机顶盒电视、数字电视。主要现在的是机顶盒电视。机顶盒电视有个问题就是电视关了，但是机顶盒还在传信号。厂家那边的数据不是为了你研究的，他会忠实认为电视关了，机顶盒传信号的情况下，这家人还在看电视。这什么大数据，显然**傻数据**啊。这显然有问题。机顶盒电视的晚上收视率会远远大于数字电视，这个怎么办？

2、这几个大厂的数据能代表整个电视市场吗？加入很多农村用的都是各自省份的地方二线山寨品牌的电视，这方面数据永远拿不到。传统行业不像互联网一样赢家通吃，很多厂家在一个行业，各自定位不同，群体有各自的定位，导致样本很难随机。假如你拿到的是乐视的大数据，那么很多乐视很多是智能电视，很多年轻人用，各种偶像剧和综艺收视率高，不怎么看新闻联播。尽管这方面乐视的用户数据是详尽的，但是广大农村那些天天看新闻联播的人群大数据就不知道了，而且很难拼接推测出来。

3、没有人口统计的基本数据，厂家有用户如何使用电视的数据，但是他没有用户的收入、家庭人员数、性别、年龄这些基本的统计量。这让控制变量和拼接数据也变得困难，各种美女的碎片很难拼接。

改善办法也有，简单来说，就是靠算法。  
1、通过小数据精确了解当地的数据，加上大数据详尽了解全国的数据，互相拼凑得出结果。问题是，这样的话，调整算法的参数，基本想要什么样的数据就可以得出什么数据。用上海的数据推全国，看新闻联播的太少？那假设 GDP 和看新闻联播的人成正比，那些地方 GDP 低，恩再算一下，哦，新闻联播收视率上来了。各种这些操作都很容易进行，可以生产 N 个版本的数据，而且统计上慢慢做，总能显著的。问题是，不知这么做可能的风险啊？

2、大数据内部算法修改。例如晚上 2 点以后长期不换台的人显然是关了电视，没关机顶盒的。我识别这批用户，然后修改他们看电视数据。例如，总看儿童节目的人，家里一定有小孩。找找普查的人口结构数据，推测我的用户情况。但是这样的修改，依旧会对最后结果巨大影响。

**因此，目前来看，大数据还不能完全取代科学随机抽样的小数据，但是可以取代很多不严谨随便在大街上找几个人调查的商业调查。学术上，大数据和小数据相结合推测整体的技术在发展，但是如何可控可靠，还是要靠普查数据保底。**

对于商业研究来说，预测就好，不讲数据后面的过程，大数据很多时候有奇效，但是也容易跳坑。**例如，很多时候发现，鞋子越大，儿童智商越高，越需要教育培训机构。所以教育培训机构多和鞋厂合作？显然不多，鞋子越大，儿童年龄越大，自然智商越高，越需要教育培训机构。大数据后面大量的这种假相关。**  
对于严谨的学术研究来说，需要讲清楚数据后面的过程，理论，大数据处于学者积极尝试的领域，但是整体是谨慎的。试试么，万一有可以讲出新理论、新知识？就可以发表评教授了呢！  
至于很多初次涉入这个领域的个人，特别是在中国这个数据如此不透明的社会，自己从网上或者厂家那弄数据总比自己在街上找几百个人，或者用问卷星在微信微博里发几份的问卷好些。个人能够掌控数据的能力增加了，也许对现在的中国有进步意义。毕竟加了数据和表格，似乎就李菊福了呢

至于类似《大数据时代》里面那种因果归因完全不重要的说法，显然扯淡。数据从来就不能直接提供因果，数据只是工具和说理的论据。他有自身缺陷，有统计和调查知识的人可以合理质疑。

**如何用数据说好道理，数据方法和研究问题契合，尽量贴近因果，这是不懈但永远到不了的追求。大数据的方法，只是这个进步中容易引普通人和投资人注意的一步而已。**

  

**求赞！作为文科生做学编程，做数据工作不容易！**

  
**ps：这篇收视率研究的论文，公开发表后，我会尽快加上作者和参考文献的。毕竟讲座听到的，很多信息不完备。**
    
    
    
    
### 知乎用户 鲍宇辰 发表
    
补充一点自己的观察。  
二者的关系并非互相替代，而是相互补充。  
Survey Data 的目的是要做 Statistical Inference，而这背后是统计学深厚的积淀。  
Big Data 则是提供巨量数据的工具（以及分析），相关关系较容易获得，但本身没有办法做因果推论，而且样本和母体间的关系也是暧昧不清。  
因此我认为在 Big Data 的语境下，有一种方式可能是对传统 Survey 体系的挑战。  
**那就是直接做社会的随机实验，而不再去考虑抽样的问题。**  
众所周知，社会科学研究无法像自然科学研究一样，通过控制法来做实验。因而才求助于统计，延伸了社会科学研究的脉络，甚至也衍生出了类似反事实（Counterfactual）进行因果推论的方法。其出发点，都是为了像控制法靠拢。  
但随着网际网路和社群媒体的出现和成熟，控制实验法开始出现了进入社会科学的可能。  
我想举一个例子，是 2013 年 Nature 的一篇论文。  
研究者使用 Facebook 上 6100 万用户做了一个关于社会影响力的实验，主要是看 2010 年美国国会选举期间，好友间的政治动员是否会影响到他人的投票行为。稍微有点儿感觉的人都知道，这个实验的人数是现实生活中根本做不到的，我在台湾看的话，一般的 survey data 样本也就一千到几千，大陆厉害一些，比如谢宇主持的 CFPS 第一期数据成人和儿童样本有 5 万多人，已经算相当多的了。  
回归正题，这些人被分为 3 组，1% 被分到 information message group，98% 被分到 social message group，还有 1% 的人被分到 control group。  
social message group 的信息页上会有一个特殊的信息栏（如图 1 所示），鼓励人们去投票。它会显示附近的投票点，并设置了一个 “我投过票了” 的按钮，还能看到有多少 Facebook 用户已经点过这个这个按钮，以及你有多少朋友已经点过这个按钮（至多显示 6 位）。  
information message group 也可以看到这个信息栏，但是看不到任何朋友的点击信息。control group 完全看不到这个信息栏。  



![](https://images.weserv.nl/?url=https%3A//pic2.zhimg.com/32955cf924aa49971f0bc00a4f18b6b4_r.jpg%3Fsource%3D1940ef5c)

图 1，来源：Nature. [http://www.nature.com/nature/journal/v489/n7415/carousel/nature11421-f1.2.jpg](https://link.zhihu.com/?target=http%3A//www.nature.com/nature/journal/v489/n7415/carousel/nature11421-f1.2.jpg)

从图 1 中的图表可以看到，接收到 social message 的人相较于接收到 information message 的人来说，多出了 2.08% 的人点了 “我投过票了” 按钮并在统计上显著（t-test, P<0.01）。同时，social 组的人也更可能会查询附近的投票点。（多 0.26%，t-test, P<0.01）  
看起来也就这样，对吗？如果这篇论文只停留在这里的话，它是不可能登上 Nature 的。  
有趣的是，研究者使用了公开的投票信息与 Facebook 上的用户点击 “我投过票了” 的行为进行比对验证，他们验证了整个参与实验的人的 10%，大约 630 万人。（美国部分州是要求选民同意在投票后公开其姓名信息，当然不会公开投票内容，只是告诉别人，确实有个张三在我们这儿投过票；另外，Facebook 基本算个实名社群媒体）  
通过比对的结果，研究者发现接收 social message 的人比控制组的人多出了 0.39% 的投票行为（t-test, P=0.02）。同样的，接收 social message 的人也比接收 information message 的人多出了 0.39% 的投票行为（t-test, P=0.02）。  
也就是说，**朋友脸书的投票内容对用户在真实世界中的投票行为有显著的正向影响**。  
这个结论是惊人的，也是新的实证研究的魅力所在。在过往的投票行为研究中，只能通过问卷中的一些逻辑问题来判断样本是否有效，基本上还是受访者说了算，想说谎没人能制止。如今，可以借用大量用户和巨量数据来进行线上实验和线下验证，这对于社会科学研究来说无疑是颠覆性的。  
但可惜的是，想做这样激动人心的实验，你得先进去 Facebook 再说。从资讯社会学的理论来说，涉及到了权力的转移，社会权利从政府手中流转到提供网络服务的大公司、大企业手中。  
说到这儿，我们可能会问一个问题：我们的数据，可以被随意拿来供 Facebook 来做实验吗？  
答案是肯定的，因为 Facebook 的用户协议已经加入了 research 这一项，尽管以前是没有的。  
关于 data 的所有权问题是个极其复杂的问题，目前看来还没什么解决之道，只能任由大互联网公司野蛮生长，这时候就只能寄希望于他们的自律精神。当然，像 Google 已经成立了数据伦理委员会，英国也在其国家机器中建立了类似的组织，但是什么时候能惠及全球，那可真没谱。  
最后扯远了，不过就这样吧。

参考文献：  
Bond, R. M., Fariss, C. J., Jones, J. J., Kramer, A. D. I., Marlow, C., Settle, J. E., & Fowler, J. H. (2012). A 61-million-person experiment in social influence and political mobilization. _Nature_, _489_(7415), 10.1038/nature11421. [http://doi.org/10.1038/nature11421](https://link.zhihu.com/?target=http%3A//doi.org/10.1038/nature11421)
    
    
    
    
### 知乎用户 chenjunrui 发表
    
首先，明确个人观点，大数据来了以后，对于传统调研数据来说，是补充 + 部分替代，而不是完全替代。  
用大数据的方法的确可以解决部分原有使用传统调研数据解决的问题，这是新技术的优势。就好像大家以前看电影，等电视出来以后，的确部分用户被吸引到电视上了，但是并不能完全替代掉电影。所以，在很长一段时间，在一些具体场景、方法上，大数据是无法替代掉传统调研数据的。

另外，说是补充，则是因为大数据的确开创了一些数据使用的方法，帮助研究更好的理解。比如通过移动终端研究城市每天的人口流动情况，这在以前很难用传统调研方法实现（理论上来说，我认为所有的大数据研究都可以用传统调研数据的方法来实现，只是成本超乎想象。你就想象一下，大数据其实就是选择了全体样本，并为每一个样本配置了一个拥有各种设备的调查员进行记录的传统调研数据）。

我认为，大数据对于研究来说，带来的变化主要会体现在三个层面上:

大数据对于近期研究的影响我认为主要是研究思维的变化。  
从源头分析，大数据其实是随着信息技术的发展的。在信息服务过程中，本身产生了很多数据，这些数据是为了完成信息服务而产生的，当把这些数据汇集的时候，这些数据中本身可能包含着一些具体的有价值的信息。某种意义上可以说大数据是一种过程中的数据。  
与此相对，调研数据更多的是一种结果中的数据，是为了研究而研究的数据。如果不是为了研究结果，其实是没有必要去做调研的。  
所以，我认为大数据对于研究的改变是一种研究思维的改变，当你拿到一个项目后，以前是分析项目，确定项目目标，确定数据需求，制定调研计划，抽样标准，实施抽样，调研，分析；现在则应该加上一个维度：分析项目，确定项目目标，确定数据需求，从信息服务过程的数据中截取数据，分析。  
也就是说你需要考虑一种新的数据获取的方案。

另一种变化则是对数据价值认知的变化.  
其实国外很多有价值的社会研究，往往是设计了一个有意义的假设，并通过某个机构曾经有过的几十年的记录档案进行了验证，并得出了结论。而这些记录档案原本并不是为了这种研究服务的（这是不是有一点点前面提到的大数据的影子呢）  
因此，这意味着在大数据时代，数据的价值需要重新被认识，所有的数据都有可能有未被发现的价值，对你来说没有的数据可能就是对其他人来说非常宝贵的材料。这就意味着需要尽可能保存数据，而无限的数据量相对于有限的技术能力而言，保存哪些数据成为了一个重要问题。

第三种变化则是更长远的变化，涉及到研究方法的改变。  
随着数据量的增多，事实上人脑的分析能力可能会不够用了。  
1、以前分析问题的时候，研究思路是：数据，模型。这个阶段，操作和设计都是人脑制定的。  
2、随着数据量的增加，研究思路变成：数据，模型，模型脚本。这个阶段，执行已经变成了机器，而设计还是由人脑制定的。  
3、但是随着数据进一步加大，研究思路预计可能会变成：数据、模型，模型脚本，模型模式。这个阶段，人脑只是提供了一种规则，而执行和设计已经由机器自动判断和执行了。  
我认为，随着大数据的发展，第三种研究方法可能会逐步得到应用，这就是整体的研究方法的改变了。

从上述内容推论，  
未来最终数据都会进入到一个数据集市中。供所有人使用。在这种大数据的数据集市下，人们不可能处理所有的数据，所以大量的数据处理机器人会产生，他们会不断地在数据集市中寻找模式，并输出给相应的人，以供进一步分析。研究就好像类似于现在的 google，但是你输入的问题可能是 “国家 GDP 与哪些因素有关”，然后通过大数据分析引擎，可能会给出你一堆各种与 GDP 高度相关因素的数据和分析。
    
    
    
    
### 知乎用户 ZRLi 发表
    
个人感觉 survey data 本身受限于成本很难有多大，但是 survey 之外 unsolicited 的 big data 能帮助 survey design 和 analysis，比如最近关注度突然上升的 informative sampling。

不过目前来看似乎还没有什么成功的实际应用起来的例子（？）。。。前排占坑围观 (← ←)
    
    
    

