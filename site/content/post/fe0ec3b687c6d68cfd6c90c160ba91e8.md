---
layout: default
Lastmod: 2023-02-09T18:34:35.504488+00:00
date: 2023-02-09T18:34:35.504431+00:00
title: "中国如何缺席ChatGPT盛宴"
author: "红博士"
tags: [GPT,OpenAI,AI,模型,ChatGPT]
---

目录：  

1.  ChatGPT 编年史
    
2.  我们如何错过GPT盛宴
    
3.  GPT大语言模型能实现AGI吗
    
4.  连载话题预告
    

### ChatGPT编年史

我们来梳理一个时间轴。ChatGPT是对话式UI + GPT–3.5系列模型，我们以最具代表性的论文、模型、API为主线，梳理到今天。

  

2020之前

\- 2017年6月，Google发布Transformer论文。

\- 2017年6月，7月，OpenAI发布人类喜好的强化学习算法、PPO算法，都是ChatGPT用到的算法。

\- 2018年6月，OpenAI发布GPT-1.

\- 2018年11月，Google发布BERT，此后NLP领域主要基于这个框架研究下游任务。

\- 2019年2月，OpenAI发布GPT-2，OpenAI获得了自信，此后专注于GPT.

  

2020年

\- 年初，Covid-19爆发。**中国闭关**。

\- 1月，OpenAI发布语言模型的Scaling Law（概念：模型能力跟参数规模、数据规模强相关），OpenAI获得了在数据和参数规模上Scaling-up的信心。

\- 5月，GPT-3论文发布。

\- 6月，**GPT-3 API发布**。

\- 9月，ChatGPT的关键原型算法相关论文发布。

\- 12月，欧洲机构发布用于GPT-3复现的开源数据集。

  

2021年

\- 7月，OpenAI发布Copilot原型算法。

\- 8月，Codex API发布。

\- 11月，**GPT-3 API Public Release，不对中国开放**。

\- **中国闭关**。

  

2022年

\- 1月，GPT-3.5 API (text-davinci-002)发布，该模型经过Github代码的训练加持，推理能力显著提升（该假设的因果关系待学术界论证），经过Alignment技术的加持，Follow人类指令的能力显著提升，输出结果有用性和无害性显著提升。

\- 3月，GPT-3.5论文发布，公开Alignment算法。

\- 5月，OpenAI Codex已经被70个应用使用，包括微软收购的Github的Copilot.

\- 8月，Stability AI开源StableDiffusion，文生图的算法的效果可用、速度可行、代码开源同时发生，引爆图片生成。一时间，在中国，AIGC似乎就是图片生成的代名词。

\- 9月，Sequoia Capital发布Generative AI: A Creative New World博客。

\- 中国研究人员和开发者，没有OpenAI的API权限。但图片生成却人人都可以尝试，于是互联网似乎只注意到了图片生成，对GPT大语言模型的关注度进一步下降。

\- 经过接近一年的API接入和UI探索、近一年的思维链（Chain of Thought）等Prompt Engineering技术试错、模型加速等技术（如Flash Attention、Fixed-Point）带来的成本和延迟下降，GPT-3.5的模型潜力得到开发（变得Better、Faster and Cheaper）, Copy.ai, Jasper等文本生成类公司的产品逐渐成熟。

\- 11月，OpenAI发布GPT3.5 API的新模型(text-davinci-003).

\- **12月1日，ChatGPT发布**。Musk等名流开始谈论ChatGPT，引爆英文互联网。

\- 12月初，中国互联网的自媒体逐渐开始讨论ChatGPT，主要以翻译twitter的方式。知乎上有学者开始反思。一周后，关注指数下降，两个月来只剩下AI自媒体把ChatGPT作为自己的主要关注内容。

\- **中国闭关**。

  

2023年

\- 1月，微软宣布投资OpenAI数十亿美元，并将GPT加入全家桶。

\- 2月，中国春节结束，微软和Google你方唱罢我登场，纳斯达克财报季，AI被反复提起。中国互联网是认识微软的，ChatGPT引爆中国互联网，关注指数飙升。

\- **中国开放**。

值得注意的是，中国因为疫情闭关的三年，正是OpenAI的GPT发展、壮大、产品化的三年。

### 我们如何错过GPT盛宴？

历史回顾完了，那么为什么我们（中国，尤其是AI社区）没有更早地意识到，OpenAI技术在应用层面的突破性？

意识到问题需要同时具备哪些条件：

1\. 能够看且懂OpenAI、DeepMind、Google等机构的论文（代表人群：研究员）

2\. 能够使用OpenAI的API探索论文里的模型 （代表人群：研究员里的尝鲜者）

3\. 对硅谷的敏感性，经常看大家在用OpenAI的API做什么产品 （代表人群：VC）

这三类人在中国，我们粗估一下，第一类，大概有1/100,000，第二类大概是第一类里的1/1,000，第三类大概是1/1,000,000. 三个条件，缺少一个，都无法意识到OpenAI发展到哪一步了。有哪个团队汇集了这三种人，并且他们有充分的碰撞？有哪个人是具备了这三种属性？ 雪上加霜的是，研究人员三年来被封在国内，没有出国参加过学术会议交流，甚至我猜很多人连线上会议都没有参加，很多东西我们从论文上是看不到的。

我们继续深挖。第一类人群中，又分成NLP（自然语言处理）研究人员，其他AI研究人员（比如计算机视觉、语音识别、机器学习）。

中国NLP的研究群体里，基本上是把语言模型（尤其是BERT，而不是GPT）拿去应用在NLP的各种下游任务上，在学术界就是刷榜发论文，在工业界，就是拿去做客服机器人、写稿机器人、角色扮演机器人，研究方法也完全不同于GPT精髓——Scaling-up和Alignment。（几乎）没有人是把大语言模型（LLM）当做通用人工智能（AGI）的一种可能性来研究的。

其他AI研究人员，比如计算机视觉，大部分人还是专注在图像上，即使是用Transformer，也是解决图像的问题，比如用Transformer来做自动驾驶、图像生成等。即使是Tesla AutoPilot的AI主管Karpathy。Karpathy在2022年上半年从Tesla裸辞，以独立研究员的身份，投身于大语言模型。

Karpathy曾经说他过去十年痴迷于AI中取得最快进展的方向，并且曾经对语言模型非常感兴趣，但是却忽视了scaling up的力量，那就是简单的Objective（next word）+简单的结构（Transformer）+ 足够的参数+足够的数据(web text)，一个语言模型可以涌现出在小规模状态下看不到的能力，他曾像其他人一样（他应该指早期的OpenAI），一度以为强化学习是AGI的路径，到头来却发现大语言模型是看起来最有希望的路径。在此之前，语言模型的研究人员，把精力过多地放在了具体任务上。

再说AI领域的另一个重要群体——计算机视觉（Computer Vision)群体。在2012年开始的深度学习浪潮里，计算机视觉一直是应用最广、商业化最成功的方向，吸引了太多AI研究员的精力，从图像分类、检测、分割到识别，从图像到视频，从高层视觉到底层视觉，我们在卷积神经网络上卷出了一个又一个新高度。一个YOLO目标检测框架，被迭代到原作者都放弃了，还有人给推到了v7版本。最具代表性的是计算机视觉的登月工程——自动驾驶，它需要成像、识别、合成、建图、规划等几乎所有的视觉AI技术加持，从CNN时代到Transformer时代，不断地拉更多的人下水，但直到今天，全自动驾驶的方案仍未收敛。马斯克定义的问题是对的，自动驾驶是一个real-world AI问题，但显然特斯拉的方案并没有为全自动驾驶准备好。

NLP圈的小家碧玉，CV圈的隔行隔山，疫情闭关三年，互联网信息不通。这些因素叠加起来，整个中文世界，形成了一个信息茧房。10年来，我们以为自己积攒的AI算法、数据、应用的优势，如今变成中美巨大的鸿沟。这个时候，我们甚至没有一个新闻调查，把这件事的来龙去脉，挖它个底朝天。

另一个问题是，我们的中文互联网不足以提供高质量的训练数据。什么是高质量的数据？比如维基百科、高质量的活跃论坛、专业新闻、学术论文、高质量代码、图书。

我们看看GPT–3的训练数据是什么。权重最大的数据集是OpenWebText（开源版本）,数据是从Reddit论坛上收集的URL，再把内容抓取下来。Common Crawl是一个开放的互联网数据存档（英文占一半，中文大概5%）。其他一些代表性的数据包括Wikipedia维基百科，Books开放图书，Stack Exchange技术问答社区，Github 代码，ArXiv论文，RealNew新闻存档，PubMed医疗数据。可以看到，由中文互联网产生的数据，比例低到可以忽略。这也是困扰很多试图训练中文大模型的问题，但实际上，ChatGPT的用中文沟通的能力，已经远超那些专门的中文大语言模型了，背后原因是GPT隐式学到的翻译能力。

没有好的中文数据，我们就只能搭全球互联网的数据顺风车。上面这些优质数据的产生，需要开放的社区，我们似乎无解。

### GPT大语言模型能实现AGI吗？

基于GPT的LLM，仅仅依赖语言，大概率无法实现AGI，而只是”通往AGI的高速公路的一个出口（Yann Lecun）“。但LLM足以把互联网基础设施搞个天翻地覆，它同时具备了Logic和Memory。Logic是推理能力，Memory是对高频知识的记忆，显然Memory可以分为片上和片外，片上有限，片外无限。下一步，我们只需要专注于把LLM的Logic推到极致，把大部分低频Memory offload到模型以外，配以搜索等查询技术，就可以实现对整个互联网前后端的重构。我们远远没有吃尽scaling-law的红利，限制我们的，只有集成电路的摩尔定律和制造能力、能源的价格、数据的获取。

集成电路方面，以Chiplet为代表的系统摩尔定律还不够，人们需要能够scaling-up的Foundry。

能源方面，太阳能和风能 + 能源存储能够解决很多问题，更加激动人心的是以Helion为代表的核聚变技术，则有机会把能源价格降低一个量级，然后更多。

数据方面，目前的GPT模型依赖互联网文本数据，这会用尽，没关系，现实世界的数据是无限的。

### 连载话题预告

今天先写到这儿。

计划中：

\- OpenAI的故事

\- AI Alignment

\- AI与资本主义

\- AI与教育

\- AGI时代的人

  

By 红博士, 2023年2月8日

